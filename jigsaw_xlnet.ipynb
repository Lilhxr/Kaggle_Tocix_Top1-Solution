{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8d298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 04:03:23.157737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "## Pytorch Import\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "##Pytorch进行优化，更新参数\n",
    "import torch.optim as optim\n",
    "##根据需求在epochs增大的时候自动降低学习率\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "## transformer \n",
    "from transformers import AdamW,AutoTokenizer,AutoModel,AutoConfig\n",
    "\n",
    "## import Scikit\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from transformers import  XLNetTokenizer, XLNetModel, TFXLNetModel, XLNetLMHeadModel, XLNetConfig, XLNetForSequenceClassification\n",
    "## color \n",
    "from colorama import Fore,Back,Style\n",
    "\n",
    "# from DeBERTa import deberta\n",
    "\n",
    "\n",
    "red = Fore.RED\n",
    "blue = Fore.BLUE\n",
    "set_all = Style.RESET_ALL\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../input/tez-lib/\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "vali = pd.read_csv('classification-pseudo-label/classification_pairs_891.csv')\n",
    "# tmp_valid = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\n",
    "\n",
    "# vali = pd.merge(vali, tmp_valid, how='left', left_on=['less_toxic', 'more_toxic'], right_on=['less_toxic', 'more_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6dcc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    model_name_or_path = '../input/xlnet-base-cased'\n",
    "        \n",
    "    epochs = 2\n",
    "    train_bs = 32\n",
    "    valid_bs = 64\n",
    "        \n",
    "    seed = 42\n",
    "    max_length = 128\n",
    "    min_lr = 1e-7\n",
    "    margin = 0.5\n",
    "#     scheduler = 'CosineAnnealingLR' # 学习率衰减策略\n",
    "    T_max  = 500\n",
    "    T_0 = 5\n",
    "    weight_decay = 1e-4 # 权重衰减 L2正则化 减少过拟合\n",
    "    max_grad_norm = 1.0 # 用于控制梯度膨胀，如果梯度向量的L2模超过max_grad_norm，则等比例缩小\n",
    "    num_classes = 1\n",
    "    n_fold = 5\n",
    "    n_accululate = 1\n",
    "    device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    hidden_size = 768\n",
    "    num_hidden_layers = 12\n",
    "    \n",
    "    dropout = 0.2\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    lr =  1e-4\n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    \n",
    "tokenizer = XLNetTokenizer.from_pretrained('../input/xlnetbasecased/xlnet_cased_L-12_H-768_A-12')\n",
    "config = XLNetConfig.from_pretrained('../input/xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0aab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89ccc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "kf = KFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "for fold, ( _, val_) in enumerate(kf.split(X=vali)):\n",
    "    vali.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "vali[\"kfold\"] = vali[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e30217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JDataset(Dataset):\n",
    "    def __init__(self, vali, tokenizer, max_length):\n",
    "        self.vali = vali\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.more_toxic = vali['more_toxic'].values\n",
    "        self.less_toxic = vali['less_toxic'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vali)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        more_toxic = self.more_toxic[index]\n",
    "        less_toxic = self.less_toxic[index]\n",
    "        ## encode_plus 返回句子所有的编码信息 \n",
    "        ## input_ids : 单词在词典中的编码   token_type_ids : 区分两个句子的编码  attention_mask : 指定哪些词进行self_attention的操作\n",
    "        ## sentence = \"Hello, my son is laughing.\"\n",
    "        ## print(tokenizer.encode(sentence))\n",
    "        ## print(tokenizer.encode_plus(sentence))\n",
    "#         [101, 7592, 1010, 2026, 2365, 2003, 5870, 1012, 102]\n",
    "#         {'input_ids': [101, 7592, 1010, 2026, 2365, 2003, 5870, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "        inputs_more_toxic = self.tokenizer.encode_plus(\n",
    "                                more_toxic,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                return_attention_mask=True,\n",
    "                                return_token_type_ids=True,\n",
    "                            )\n",
    "        inputs_less_toxic = self.tokenizer.encode_plus(\n",
    "                                less_toxic,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                return_attention_mask=True,\n",
    "                                return_token_type_ids=True,\n",
    "                            )\n",
    "        target = 1\n",
    "        \n",
    "        more_toxic_ids = inputs_more_toxic['input_ids']\n",
    "        more_toxic_mask = inputs_more_toxic['attention_mask']\n",
    "        more_toxic_token_type_ids = inputs_more_toxic['token_type_ids']\n",
    "        \n",
    "        less_toxic_ids = inputs_less_toxic['input_ids']\n",
    "        less_toxic_mask = inputs_less_toxic['attention_mask']\n",
    "        less_toxic_token_type_ids = inputs_less_toxic['token_type_ids']\n",
    "        \n",
    "        return {\n",
    "            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n",
    "            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n",
    "            'more_toxic_token_type_ids':torch.tensor(more_toxic_token_type_ids,dtype=torch.long),\n",
    "            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n",
    "            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n",
    "            'less_toxic_token_type_ids':torch.tensor(less_toxic_token_type_ids,dtype= torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45408c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetBaseModel(nn.Module):\n",
    "    def __init__(self, checkpoint=Config.model_name_or_path):\n",
    "        super(XLNetBaseModel, self).__init__()\n",
    "        self.checkpoint = checkpoint\n",
    "        self.xlnet = XLNetModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.layer_norm = nn.LayerNorm(Config.hidden_size)\n",
    "        self.dropout = nn.Dropout(Config.dropout)\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(Config.hidden_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(Config.dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids,token_type_ids, attention_mask):\n",
    "        last_hidden_state = self.xlnet(input_ids=input_ids,token_type_ids = token_type_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.pool_hidden_state(last_hidden_state)\n",
    "        pooled_output = self.layer_norm(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        preds = self.dense(pooled_output)\n",
    "        return preds\n",
    "    \n",
    "    def pool_hidden_state(self,last_hidden_state):\n",
    "        '''\n",
    "        pool the last_hidden_state into a mean hidden_state\n",
    "        '''\n",
    "        last_hidden_state = last_hidden_state[0]\n",
    "        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n",
    "        return mean_last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5ea417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(output1, output2, targets):\n",
    "    return nn.MarginRankingLoss(margin=Config.margin)(output1, output2, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d235d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=4):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"],\n",
    "                    float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e64944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    \n",
    "    #启用batch normalization和drop out  \n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    \n",
    "    for step, data in bar:\n",
    "        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n",
    "        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n",
    "        more_token_type_ids = data['more_toxic_token_type_ids'].to(device, dtype = torch.long)\n",
    "        \n",
    "        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n",
    "        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n",
    "        less_token_type_ids = data['less_toxic_token_type_ids'].to(device, dtype = torch.long)\n",
    "        \n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = more_toxic_ids.size(0)\n",
    "\n",
    "        more_toxic_outputs = model(more_toxic_ids,more_token_type_ids, more_toxic_mask)\n",
    "        less_toxic_outputs = model(less_toxic_ids, less_token_type_ids,less_toxic_mask)\n",
    "        \n",
    "        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n",
    "        # 梯度累加\n",
    "        loss = loss / Config.n_accululate\n",
    "        loss.backward()\n",
    "    \n",
    "        if (step + 1) % Config.n_accululate == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1444b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    #神经网络会沿用batch normalization的值，并不使用drop out\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    \n",
    "    for step, data in bar:        \n",
    "        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n",
    "        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n",
    "        more_token_type_ids = data['more_toxic_token_type_ids'].to(device, dtype = torch.long)\n",
    "        \n",
    "        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n",
    "        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n",
    "        less_token_type_ids = data['less_toxic_token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = more_toxic_ids.size(0)\n",
    "\n",
    "        more_toxic_outputs = model(more_toxic_ids,more_token_type_ids, more_toxic_mask)\n",
    "        less_toxic_outputs = model(less_toxic_ids,less_token_type_ids, less_toxic_mask)\n",
    "        \n",
    "        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])   \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d125e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_of_each_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02187f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n",
    "    # To automatically log gradients\n",
    "#     wandb.watch(model, log_freq=100)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=Config.device, epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=Config.device, \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "\n",
    "        # deep copy the model\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"{red}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"Loss-Fold-{fold}.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(f\"Model Saved{set_all}\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00345ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(fold):\n",
    "    df_train = vali[vali.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = vali[vali.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = JDataset(df_train, tokenizer=tokenizer, max_length=Config.max_length)\n",
    "    valid_dataset = JDataset(df_valid, tokenizer=tokenizer, max_length=Config.max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.train_bs, \n",
    "                              num_workers=8, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=Config.valid_bs, \n",
    "                              num_workers=8, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b8f0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if Config.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=Config.T_max, \n",
    "                                                   eta_min=Config.min_lr)\n",
    "    elif Configscheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=5, \n",
    "                                                             eta_min=Config.min_lr)\n",
    "    elif Config.scheduler == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcb69bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "    roberta_parameters = named_parameters[:197]    \n",
    "    attention_parameters = named_parameters[199:203]\n",
    "    regressor_parameters = named_parameters[203:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "        lr = Config.lr\n",
    "\n",
    "        if layer_num >= 69:        \n",
    "            lr = Config.lr * 2.5\n",
    "\n",
    "        if layer_num >= 133:\n",
    "            lr = Config.lr * 5\n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "\n",
    "    return optim.AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "586166cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m====== Fold: 0 ======\u001b[0m\n",
      "[INFO] Using GPU: NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:25<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:25<00:00,  3.22it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mValidation Loss Improved (inf ---> 0.1133230497876803)\n",
      "Model Saved\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:25<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:25<00:00,  3.22it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mValidation Loss Improved (0.1133230497876803 ---> 0.09854290266036987)\n",
      "Model Saved\u001b[0m\n",
      "\n",
      "Training complete in 1h 3m 49s\n",
      "Best Loss: 0.0985\n",
      "\n",
      "\u001b[34m====== Fold: 1 ======\u001b[0m\n",
      "[INFO] Using GPU: NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:26<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:25<00:00,  3.22it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mValidation Loss Improved (inf ---> 0.10242218701839446)\n",
      "Model Saved\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:24<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:25<00:00,  3.22it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mValidation Loss Improved (0.10242218701839446 ---> 0.09556384704113007)\n",
      "Model Saved\u001b[0m\n",
      "\n",
      "Training complete in 1h 3m 49s\n",
      "Best Loss: 0.0956\n",
      "\n",
      "\u001b[34m====== Fold: 2 ======\u001b[0m\n",
      "[INFO] Using GPU: NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:27<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:26<00:00,  3.21it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mValidation Loss Improved (inf ---> 0.099626016664505)\n",
      "Model Saved\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:30<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:25<00:00,  3.22it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 1h 3m 56s\n",
      "Best Loss: 0.0996\n",
      "\n",
      "\u001b[34m====== Fold: 3 ======\u001b[0m\n",
      "[INFO] Using GPU: NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:25<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:25<00:00,  3.21it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mValidation Loss Improved (inf ---> 0.10262608381708463)\n",
      "Model Saved\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:28<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:25<00:00,  3.21it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 1h 3m 52s\n",
      "Best Loss: 0.1026\n",
      "\n",
      "\u001b[34m====== Fold: 4 ======\u001b[0m\n",
      "[INFO] Using GPU: NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:26<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:31<00:00,  3.10it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mValidation Loss Improved (inf ---> 0.09828754281202952)\n",
      "Model Saved\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [29:29<00:00,  2.12it/s, \n",
      "100%|█| 469/469 [02:31<00:00,  3.10it/s, Ep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 1h 4m 4s\n",
      "Best Loss: 0.0983\n",
      "\n",
      "CPU times: user 10h 43min 46s, sys: 4min 44s, total: 10h 48min 31s\n",
      "Wall time: 5h 19min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for fold in range(0, Config.n_fold):\n",
    "    print(f\"{blue}====== Fold: {fold} ======{set_all}\")\n",
    "\n",
    "    # Create Dataloaders\n",
    "    train_loader, valid_loader = prepare_loaders(fold=fold)\n",
    "#     config  = AutoConfig.from_pretrained(Config.pretrained_model_path)\n",
    "#     transformer = AutoModel.from_pretrained(Config.pretrained_model_path, config=config) \n",
    "    \n",
    "    model = XLNetBaseModel(Config.model_name_or_path)\n",
    "    model.to(Config.device)\n",
    "    \n",
    "    # Define Optimizer and Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)\n",
    "    scheduler = fetch_scheduler(optimizer)\n",
    "    \n",
    "    model, history = run_training(model, optimizer, scheduler,\n",
    "                                  device=Config.device,\n",
    "                                  num_epochs=Config.epochs,\n",
    "                                  fold=fold)\n",
    "    \n",
    "    del model, history, train_loader, valid_loader\n",
    "    _ = gc.collect()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e4775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
